* Fix norb_maxout branch!
** Merge in norb branch so that browse_norb works
** See what was missing the NORB labels, fix whatever script generated it
** Confirm that view_confusion_matrix and browse_norb both work
** Put a no-arg script in demos/
** Commit and tag.
* Push norb branch
** Write a few unit tests, commit, push
* Visualizing high-level features
** Visualize sparsity
*** Change sparsity-viewing script to output codes to a file.
*** SVD codes down to <# classes> dimensions, view.
*** Try btSNE'ing the codes instead of SVD, view.
**** If we're viewing the second-to-last layer's output, is this meaningful? The next layer can only do a linear projection, like SVD, not whatever t-SNE does.
**** May be useful to t-SNE each layer's output to see if separability is increasing. Use as a cue to see if additional layers are needed.
*** Sort rows by class, then instance. See if you see any structure.

* Consider making a new .yaml from "scratch", featuring kernel sizes and strides suggested in pylearn2 tutorial docs (certainly not 24x24 pooling).

* Need to train on big NORB; small norb too fragile.
** First round: no flips or windowing; just get something working (fiddle with convnet geometry).
** Second round: 4-corners and center & horizontal flip windowing (10 images per image). No need to multiscale yet.
*** Just bite the bullet and modify windowing preprocessor to take an output argument.
*** Don't even bother ZCA'ing; GCN only.

* Update new_norb.py in norb branch, push to PR.
** In norb branch, refactor viewer code to shut that guy up. Email dev list.

* Pierre paper (classification + position regression):
** Classification:
*** Input: whole image, with possibly multiple things.
*** Output: top N most likely object classes, without locations.
** Training classifier:
*** Input: an image of random size, with labeled bounding boxes.
*** Resize input so that smallest dimension is 256.
*** Extract 5 random crops of size 221x221, and their horizontal flips.
*** Train convnet on mini-batches of size 128 of these crops.
** Running classifier:
*** Simplest (no pool-interleaving, single scale):
**** Run the classifier convnet as a sliding window. Last convoutional layer in classifier is a 5x5x1024 feature map. This goes to a FC layer yielding a 4096-D vector. Another FC layer gives a 4096D vector, and a FC softmax gives 1000-D output (1000 classes).
**** Slide that 5x5 window around the final feature map, one pixel at a time. Because there's no subsampling after the final conv layer, this is equivalent to treating the classification layers as Nx1x1 conv layers, and sliding the 1x1 classification "window" one pixel at a time. (This slides the 5x5 window one pixel at a time, not 5 pixels at a time).
**** They say they just take the "spatial max" over the image for each class. I'm guessing this just means that P(class c) = max_over_feature_map_pixels_i P(c, i).
*** Multi-scale, flips:
**** Do the above for multiple scales, and horizontal flips. This gives you a single 1000-D classification vector for each scale, flip.
**** Take the average of these classification vectors.
**** Choose the top-1 most likely class (or the top-5, if that's the competition format).
** Localization:
*** Input: Image, possibly multiple things.
*** Output: boxes around things, with classification of box contents.
** Training localizer:
*** Uses 2 fully-connected regressor layers on top of the final conv. layer:
**** 5x5x1024 -> 4096 -> 1024
*** The final layer maps from 1024 -> 4, to define the 4 corners of the bounding box.
**** This last layer can be class-specific, though they found that it does better when it isn't.
**** I assume the 4 units correspond to opposing bbox corners given by x1, y1, x2, y2, but don't know for sure.
**** Trained with a L2 loss wrt ground truth box.
*** Train at all scales, all sliding window locations.
** Running localizer:
*** Choose a priori K, the max # of classes you want to detect. Note that this is not the max # of objects, since there can be multiple instances of each class.
*** For each scale s,
**** Compute Cs, the set of most likely classes seen at scale s
***** Compute classification vectors for all sliding windows
***** For each class, take the max probability over all windows at this scale.
***** Store the top-K probable classes for this scale as Cs.
***** Note that Cs can have more than K classes, since different windows can provide different top-K classes.
**** Compute set of bounding boxes Bc for each class c in Cs:
***** For each sliding window in s:
****** Run classifier to get classification vector V. Run regression layers (last layer possibly c-specific) to get bounding box b. Add b to set Bc, setting its class confidence score to V[c].
*** Reduce Bc by merging similar boxes within it. Merged boxes get the confidences of their component boxes.
**** Merge boxes in Bc's by repeating until break:
***** find closest pair of bboxes b1, b2 in Bc, according to match_score(b1, b2)
****** match_score(b1, b2): A + D
******* A: intersection area
******* D: distance btw. box centers.
******* This penalizes boxes that are too big or too far to be merged. Because merged boxes get the sum of their components confidences, this effectively penalizes low-confidence outlier boxes that can't be merged to anything.
***** If match_score(b1, b2) > some_threshold: break.
***** Else: replace b1, b2 in Bc with box_merge(b1, b2)
****** box_merge(b1, b2):
******* return "average of the bounding boxes' coordinates", with class-confidences of the two boxes added together.
*** Among boxes of all Bc sets, return the boxes with highest class confidence scores.

* Training on Norb:
** Can't window or flip, because window-or-flip won't take a 
* Switch to norb branch, Get SmallNORB to use memmaps.
* Merge norb into maxout_norb, try running preprocess_small_norb.py on small NORB again.
* Once preprocess_small_norb.py is working, delete create_instance_norb_dataset.py. It's not even in git.
* Re-run preprocess_small_norb.py on small norb to make sure it still works.
* Try making preprocess_small_norb.py normalize the image pixel values to [0..1] (from 0..255), see if the training works better or worse.
* ZCA can't handle the size of full NORB. Is there a way to fix this? Find the numpy functions involved, and google for how people run them on large matrices.
