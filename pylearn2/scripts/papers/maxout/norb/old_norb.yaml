!obj:pylearn2.train.Train {
    dataset: &train !obj:pylearn2.scripts.papers.maxout.norb.load_norb_instance_dataset {
      dataset_path: "${PYLEARN2_DATA_PATH}/norb/instance_recognition/norb_left_02_00_gcn_zca_train.pkl",
      convert_to_one_hot: True
    },
    model: !obj:pylearn2.models.mlp.MLP {
    # cuda-convnet is optimized only for batch sizes that are multiples of
    # 128. See: http://deeplearning.net/software/pylearn2/library/alex.html We
    # can't use 128 here, since we run out of memory.  NORB image vectors are
    # ~3x the size of cifar ones, so try 128/3 ~= 42 instead.
        batch_size: 42, 
        layers: [
                 !obj:pylearn2.models.maxout.MaxoutConvC01B {
                     layer_name: 'h0',
                     pad: 3,  # this should be half of the kernel size
                     tied_b: 1,
                     W_lr_scale: .05,
                     b_lr_scale: .05,
                     num_channels: 96,  # must be a multiple of 16
                     num_pieces: 2,
                     kernel_shape: [8, 8],  # must be square
                     # These were 4,4 and 2,2 for cifar10, but 3x them, since
                     # NORB images are 3x as tall and wide.
                     pool_shape: [12, 12],
                     pool_stride: [6, 6],
                     irange: .005,
                     max_kernel_norm: .9,
                     # partial_sum: 109, #97,  # was 33 for cifar10. Seems to be (image width or height) + 1.
                 },
                 !obj:pylearn2.models.maxout.MaxoutConvC01B {
                     layer_name: 'h1',
                     pad: 3,  # weird that this is 3, not kernel_shape/2
                     tied_b: 1,
                     W_lr_scale: .05,
                     b_lr_scale: .05,
                     num_channels: 192,
                     num_pieces: 2,
                     kernel_shape: [8, 8],
                     pool_shape: [3, 3], #[8, 8], # [4, 4],
                     pool_stride: [3, 3],
                     irange: .005,
                     max_kernel_norm: 1.9365,
                     # partial_sum: 17, # 15, #17, # 15
                 },
                 !obj:pylearn2.models.maxout.MaxoutConvC01B {
                     pad: 3,
                     layer_name: 'h2',
                     tied_b: 1,
                     W_lr_scale: .05,
                     b_lr_scale: .05,
                     num_channels: 192, #384, #192,  #when we use a bigger value here, we may want to specify partial_sum, to be more memory efficient
                     num_pieces: 2,
                     kernel_shape: [5, 5],
                     pool_shape: [3, 3], #[4, 4], # [2, 2],
                     pool_stride: [3, 3],
                     irange: .005,
                     max_kernel_norm: 1.9365,
                 },
                 !obj:pylearn2.models.maxout.Maxout {
                    layer_name: 'h3',
                    irange: .005,
                    num_units: 500,
                    num_pieces: 5,
                    max_col_norm: 1.9
                 },
                 !obj:pylearn2.models.mlp.Softmax {
                     max_col_norm: 1.9365,
                     layer_name: 'y',
                     n_classes: 50,
                     irange: .005
                 }
                ],
        input_space: !obj:pylearn2.space.Conv2DSpace {
            shape: &window_shape [108, 108],
            num_channels: 1,
            axes: ['c', 0, 1, 'b'],
        },
    },
    algorithm: !obj:pylearn2.training_algorithms.sgd.SGD {
        learning_rate: .17,
        init_momentum: .5,
        train_iteration_mode: 'even_sequential',
        monitor_iteration_mode: 'even_sequential',
        monitoring_dataset:
        {
        # this feels cheaty: using the test set as validation. maxout's cifar10.yaml does it, but the real thing to do is probably split the test set into two, and use one for validation and the other for testing?
          'valid' : !obj:pylearn2.scripts.papers.maxout.norb.load_norb_instance_dataset {
            dataset_path: "${PYLEARN2_DATA_PATH}/norb/instance_recognition/norb_left_02_00_gcn_zca_test.pkl",
            convert_to_one_hot: True
            },
          'test' : &valid !obj:pylearn2.scripts.papers.maxout.norb.load_norb_instance_dataset {
            dataset_path: "${PYLEARN2_DATA_PATH}/norb/instance_recognition/norb_left_02_00_gcn_zca_test.pkl",
            convert_to_one_hot: True
            },
        },
        cost: !obj:pylearn2.costs.mlp.dropout.Dropout {
            input_include_probs: { 'h0' : .8 },
            input_scales: { 'h0' : 1. }
        },
        termination_criterion: !obj:pylearn2.termination_criteria.And {
            criteria: [
                !obj:pylearn2.termination_criteria.MonitorBased {
                    channel_name: "valid_y_misclass",
                    prop_decrease: 0.01, # 0.,
                    N: 20 # 100
                },
                !obj:pylearn2.termination_criteria.EpochCounter {
                    max_epochs: &max_num_epochs 400 # 474
                }
            ]
        },
    },
    extensions: [
        !obj:pylearn2.training_algorithms.sgd.MomentumAdjustor {
            start: 1,
            saturate: 50, #*max_num_epochs, # 250,
            final_momentum: .65
        },
        !obj:pylearn2.training_algorithms.sgd.LinearDecayOverEpoch {
            start: 1,  # start decaying on this epoch
            saturate: *max_num_epochs, # 500,
            decay_factor: .01  # final learning rate = initial rate times this.
        },
        # WindowAndFlip currently makes a windowed-and-flipped copy of the
        # entire dataset in memory, so can't use it with big NORB.
        !obj:pylearn2.train_extensions.best_params.MonitorBasedSaveBest {
            channel_name: 'valid_y_misclass',
            save_path: 'norb_maxout_experiments/${PYLEARN2_TRAIN_FILE_FULL_STEM}_best.pkl'
        }
    ],
    save_path: "norb_maxout_experiments/${PYLEARN2_TRAIN_FILE_FULL_STEM}.pkl",
    save_freq: 1
}
